<!DOCTYPE html>
<html lang="en" class="sr">
<head>
    <meta charset="UTF-8">
    <title>CHAD - Kubernetes Cluster</title>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, shrink-to-fit=no, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no, target-densityDpi=device-dpi"/>
    <meta name="description"
          content="CHAD is a self-hosted, high-frequency Kubernetes cluster located in Los Angeles.">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
          integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300&family=Spectral:wght@500&display=swap" rel="stylesheet">
</head>

<body>
<div class="container">

    <div class="header" style="padding-top: 2rem">
        <div class="d-inline-block w-100">
            <h1 class="title">CHAD</h1>
            <h4 class="subheading">A self-hosted, high-frequency Kubernetes cluster</h4>
            <h4 class="subheading">Written by <a href="https://www.matte.fyi">Matthew Treadwell</a></h4>

        </div>
    </div>

    <div id="document">
        <p>
            CHAD is a self-hosted compute cluster capable of running distributed workloads and services with Kubernetes.
            The project originally grew from the single dedicated web server I hosted in my closet.
        </p>

        <div class="text-center">
            <img src="img/serverCloset.jpg" alt="Picture of server closet" class="img-fluid mx-auto">
        </div>

        <!-- For typography testing -->
        <!--<p>abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz</p>-->

        <h2 class="sectionText"><b>Motivation</b></h2>
        <p>
            Over 5 years ago, I started using Linux and hosting public-facing services from my desktop; mainly, game servers for me and my friends.
            A couple years later, I repurposed an old desktop as a dedicated machine for my services.
            Having a dedicated machine greatly increased reliability, took load off my desktop, and was very easy to manage.
            This worked out well until the machine started to idle at ~70% memory usage, which meant that adding more services wouldn't be feasible.
            Likewise, all of my services were in danger of going down if the memory spiked and ran out during intensive loads.
        </p>
        <p>
            To fix this problem, I decided to use some of the machines I had lying around and build them into a horizontally scaling cluster.
            Following this approach would allow for my services to scale while also remaining budget friendly.
        </p>

        <h2 class="sectionText"><b>Hardware</b></h2>
        <h3 class="subheading">Infrastructure Nodes</h3>
        <p>Low-power and low performance. Responsible for the high-availability Kubernetes control plane.</p>
        <ul>
            <li><i>Salieri</i> - Thinkpad X250</li>
            <ul>
                <li>i5 5300u 2c/4t</li>
                <li>8gb ram</li>
            </ul>
            <li><i>Kuro</i> - ODROID-C2</li>
            <ul>
                <li>Cortex-A53 4c/4t</li>
                <li>2gb ram</li>
            </ul>
        </ul>
        <p>Originally, I also had a Raspberry Pi 3 as an infrastructure node, but I removed it due to out-of-memory crashes.</p>
        <h3 class="subheading">Compute Nodes</h3>
        <p>High-frequency with fast single core speeds. Runs production services.</p>
        <ul>
            <li><i>Amadeus</i> - Headless Dedicated Machine</li>
            <ul>
                <li>Overclocked i5 3570k 4c/4t</li>
                <li>16gb ram</li>
            </ul>
            <li><i>Altair</i> - Main Workstation</li>
            <ul>
                <li>Ryzen 3900x 12c/24t</li>
                <li>32gb ram</li>
            </ul>
            <li><i>Gehenna</i> - Living Room PC</li>
            <ul>
                <li>Overclocked i7 4770k 4c/8t</li>
                <li>16gb ram</li>
            </ul>
        </ul>

        <p>
            This works out to a total of 20 cores, 36 threads, and 64GB of memory for compute resources, which is a huge improvement from the single dedicated machine.
        </p>

        <h2 class="sectionText">Design</h2>
        <div class="text-center">
            <img src="img/chadDiagram.png" alt="Picture of server closet" class="img-fluid">
        </div>

        <p>
            To put it simply, CHAD is stacked.
            I say stacked because high-availability load balancing is performed on the infrastructure nodes themselves (inside kubernetes).
            This design avoids the need to mess with dedicated load balancers and keeps everything simple to setup.
            In the event that Salieri goes down, Kuro functions as a simple backup.
            Otherwise, control plane requests are split evenly between Kuro and Salieri.
        </p>

        <h2 class="sectionText">Networking</h2>
        <p>
            For security purposes, I won't describe any specific network configurations.
            However, here is a list of the hardware I use for the cluster:
        </p>

        <ul>
            <li>Ubiquiti EdgeRouter Lite</li>
            <li>Juniper Networks EX2200-48P-4G</li>
        </ul>

        <p>
            Overall, everything runs on wired gigabit connections.
            This works fine and isn't prohibitively expensive compared to 10GbE.
            Latency is <1ms between all nodes.
        </p>

        <h2 class="sectionText">Services</h2>
        <p>
            CHAD runs a multitude of Open Source software and game servers, both publicly and privately.
            All of these deployments are managed under the GitOps methodology
        </p>

        <p>Here are some of the services currently running:</p>

        <ul>
            <li><i>Metallb</i> - Bare metal load balancing for Kubernetes</li>
            <li><i>Traefik</i> - Kubernetes Ingress controller</li>
            <li><i>Jellyfin</i> - Media streaming</li>
            <li><i>Minecraft</i> - Public PaperMC server</li>
            <li><i>Syncthing</i> - P2P distributed file synchronization</li>
        </ul>


        <h2 class="sectionText">Principles & Methodologies</h2>
        <h3 class="subheading">GitOps</h3>
        <p>
            In order to manage all of the services effectively, all deployments are stored declaratively in git.
            This workflow makes it very easy to keep track changes and have them continuously deployed (or rolled back) with a tool like Argo CD.
        </p>


        <h3 class="subheading">Chaos Engineering</h3>
        <p>
            Currently, chaos engineering for the cluster occurs at the platform level.
            Nodes are regularly taken offline, and pods must migrate between nodes.
            In my opinion, it's important to routinely test resilience; most systems are not 100% reliable.
        </p>

        <h2 class="sectionText">Persistence</h2>
        <p>
            All persistent storage for the cluster is handled by two tiers of NFS storage on the Amadeus node.
            The first tier is high-performance, solid-state storage and the second tier is bulk data storage.
            I find that providing multiple tiers of storage allows for optimization on a per-application basis.
        </p>

        <p>
            Ultimately, these storage services are abstracted as persistent volumes within Kubernetes.
            As a result, storage can be modified without making changes to individual applications.
            This approach will be useful when migrating to a more robust storage service, such as Ceph or GlusterFS.
        </p>


        <h2 class="sectionText">Conclusion</h2>
        <p>
            Overall, this project accomplished a lot of what I needed, while also being incredibly overkill at the same time.
            After several months of running this, service reliability now hovers around 99.7% (~2 hours down a month), which is mainly due to power and internet outages each month.
        </p>

    </div>
</div>
</body>
